{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WELCOME TO MY WEBSITE CREATE BY : Alful Laila S (NIM : 180411100079) PENAMBANGAN DATA (DATA MINING) pengertian \u200b Data Mining adalah Serangkaian proses untuk menggali nilai tambah berupa informasi yang selama ini tidak diketahui secara manual dari suatu basisdata dengan melakukan penggalian pola-pola dari data dengan tujuan untuk memanipulasi data menjadi informasi yang lebih berharga yang diperoleh dengan cara mengekstraksi dan mengenali pola yang penting atau menarik dari data yang terdapat dalam basisdata. Data mining biasa juga dikenal nama lain seperti : Knowledge discovery (mining) in databases (KDD), ekstraksi pengetahuan (knowledge extraction) Analisa data/pola dan kecerdasan bisnis (business intelligence) dan merupakan alat yang penting untuk memanipulasi data untuk penyajian informasi sesuai kebutuhan user dengan tujuan untuk membantu dalam analisis koleksi pengamatan perilaku, secara umum definisi data-mining dapat diartikan sebagai berikut Proses penemuan pola yang menarik dari data yang tersimpan dalam jumlah besar. Ekstraksi dari suatu informasi yang berguna atau menarik (non-trivial, implisit, sebefumnya belum diketahui potensial kegunaannya) pola atau pengetahuan dari data yang disimpan dalam jumfah besar. Ekplorasi dari analisa secara otomatis atau semiotomatis terhadap data-data dalam jumlah besar untuk mencari pola dan aturan yang berarti. konsep data mining \u200b *Data mining * sangat perlu dilakukan terutama dalam mengelola Data yang sangat besar untuk memudahkan aktifitas recording suatu transaksi dan untuk proses data warehousing agar dapat memberikan informasi yang akurat bagi penggunanya. Alasan utama mengapa data mining sangat menarik perhatian industri informasi dalam beberapa tahun belakangan ini adalah karena tersedianya data dalam jumlah yang besar dan semakin besarnya kebutuhan untuk mengubah data tersebut menjadi informasi dan pengetahuan yang berguna karena sesuai fokus bidang ilmu ini yaitu melakukan kegiatan mengekstraksi atau menambang pengetahuan dari data yang berukuran/berjumlah besar, informasi inilah yang nantinya sangat berguna untuk pengembangan. berikut langkah-langkahnya : Data cleaning (untuk menghilangkan noise data yang tidak konsisten) Data integration (di mana sumber data yang terpecah dapat disatukan) Data selection (di mana data yang relevan dengan tugas analisis dikembalikan ke dalam database) Data transformation (di mana data berubah atau bersatu menjadi bentuk yang tepat untuk menambang dengan ringkasan performa atau operasi agresi) Knowledge Discovery (proses esensial di mana metode yang intelejen digunakan untuk mengekstrak pola data) Pattern evolution (untuk mengidentifikasi pola yang benar-benar menarik yang mewakili pengetahuan berdasarkan atas beberapa tindakan yang menarik) Knowledge presentation (di mana gambaran teknik visualisasi dan pengetahuan digunakan untuk memberikan pengetahuan yang telah ditambang kepada user). TUGAS 1 \u200b from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df= pd.read_csv(\"penambangan1.csv\") df harga buku berat buku /gram jumlah stok perbulan jumlah terjual bulan ini 0 215319 148 110 25 1 241046 133 139 44 2 146993 73 160 48 3 37237 108 105 50 4 239236 85 128 28 5 148009 123 83 21 6 47522 118 178 50 7 65608 50 90 43 8 43415 106 152 23 9 53957 59 183 26 10 94541 52 58 25 11 213957 149 146 34 12 69540 62 173 33 13 134574 126 137 28 14 145631 96 134 28 15 194378 75 92 50 16 133493 115 132 41 17 222274 107 50 42 18 142645 49 51 23 19 169453 56 165 45 20 85688 134 197 45 21 164231 58 161 42 22 205956 106 78 48 23 126871 121 148 43 24 241294 110 177 35 25 44454 127 156 44 26 132903 57 157 30 27 94979 73 90 45 28 75817 83 95 28 29 61783 64 159 37 ... ... ... ... ... 69 109358 138 73 24 70 163947 153 72 39 71 63159 125 107 23 72 42104 109 145 46 73 80948 95 149 36 74 143200 106 189 31 75 83148 112 162 32 76 147670 144 188 30 77 105098 115 161 22 78 135910 87 70 46 79 87125 71 82 28 80 135804 95 79 42 81 30766 109 166 26 82 87804 76 190 29 83 233446 88 95 26 84 31834 97 103 21 85 54182 112 76 22 86 146553 121 59 42 87 208887 124 57 24 88 207968 101 153 35 89 249144 50 142 30 90 110491 132 141 30 91 219300 126 147 37 92 169832 95 151 33 93 167964 77 200 34 94 70991 92 118 46 95 50295 155 166 43 96 100656 92 71 23 97 212300 122 65 41 98 93128 86 67 31 from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) method harga buku berat buku /gram jumlah stok perbulan jumlah terjual bulan ini describe() count 99.000000 mean 127757.464646 std 61343.035530 min 30435.000000 25% 79288.500000 50% 124700.000000 75% 169176.000000 max 249144.000000 Name: harga buku, dtype: float64 count 99.000000 mean 96.444444 std 28.745620 min 45.000000 25% 73.000000 50% 97.000000 75% 121.000000 max 155.000000 Name: berat buku /gram, dtype: float64 count 99.000000 mean 125.414141 std 42.966529 min 50.000000 25% 88.000000 50% 130.000000 75% 160.500000 max 200.000000 Name: jumlah stok perbulan, dtype: float64 count 99.000000 mean 34.323232 std 8.778253 min 21.000000 25% 28.000000 50% 34.000000 75% 42.000000 max 50.000000 Name: jumlah terjual bulan ini, dtype: float64 count() 99 99 99 99 mean() 127757.464646 96.4444444444 125.414141414 34.3232323232 std() 61343.04 28.75 42.97 8.78 min() 30435 45 50 21 max() 249144 155 200 50 q1() 79288.5 73.0 88.0 28.0 q2() 124700.0 97.0 130.0 34.0 q3() 169176.0 121.0 160.5 42.0 skew() 0.25 -0.02 -0.06 0.18 MENGUKUR JARAK DATA mengukur jarak data numerik Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dmin=( sumni=1|xi\u2212yi|m)1m,m\u22651dmin=( sumi=1n|xi\u2212yi|m)1m,m\u22651 diman mm adalah bilangan riel positif dan xixi dan $ y_i$ adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan dman=n\u2211i=1|xi\u2212yi|dman=\u2211i=1n|xi\u2212yi| Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan dave=(1nn\u2211i=1(xi\u2212yi)2)12dave=(1n\u2211i=1n(xi\u2212yi)2)12 Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dwe=(n\u2211i=1wi(xi\u2212yi)2)12dwe=(\u2211i=1nwi(xi\u2212yi)2)12dimana wiwi adalah bobot yang diberikan pada atribut ke i. Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dchord=(2\u22122\u2211ni=1xiyi\u2225x\u22252\u2225y\u22252)12dchord=(2\u22122\u2211i=1nxiyi\u2016x\u20162\u2016y\u20162)12 dimana \u2225x\u22252\u2016x\u20162 adalah L2-norm\u2225x\u22252=\u221a\u2211ni=1x2iL2-norm\u2016x\u20162=\u2211i=1nxi2 Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dmah=\u221a(x\u2212y)S\u22121(x\u2212y)Tdmah=(x\u2212y)S\u22121(x\u2212y)T diman SS adalah matrik covariance data. Cosine measure \u00b6 Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\u2211ni=1xiyi\u2225x\u22252\u2225y\u22252Cosine(x,y)=\u2211i=1nxiyi\u2016x\u20162\u2016y\u20162 dimana \u2225y\u22252\u2016y\u20162 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan \u2225y\u22252=\u221ay21+y22+\u2026+y2n\u2016y\u20162=y12+y22+\u2026+yn2 Pearson correlation \u00b6 Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson(x,y)=\u2211ni=1(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u221a\u2211ni=1(xi\u2212yi)2\u221a\u2211ni=1(xi\u2212yi)2Pearson(x,y)=\u2211i=1n(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u2211i=1n(xi\u2212yi)2\u2211i=1n(xi\u2212yi)2 The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary \u00b6 Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan jj adalah d(i,j)=r+sq+r+s+td(i,j)=r+sq+r+s+t Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d(i,j)=r+sq+r+sd(i,j)=r+sq+r+s Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan sim(i,j)=qq+r+s=1\u2212d(i,j)sim\u2061(i,j)=qq+r+s=1\u2212d(i,j) Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical \u00b6 Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric \u00b6 Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d(x,y)=n\u2211i=1\u03b4(ai(x),ai(y))d(x,y)=\u2211i=1n\u03b4(ai(x),ai(y)) dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y)ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) \u00b6 VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d(x,y)=n\u2211i=1C\u2211c=1|P(c|ai(x))\u2212P(c|ai(y))|d(x,y)=\u2211i=1n\u2211c=1C|P(c|ai(x))\u2212P(c|ai(y))| dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d(x,y)=C\u2211c=1|P(c|x)\u2212P(c|y)|d(x,y)=\u2211c=1C|P(c|x)\u2212P(c|y)| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x)P(c|x) dan P(c|y)P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan d(x,y)=C\u2211c=1P(c|x)(1\u2212P(c|y))d(x,y)=\u2211c=1CP(c|x)(1\u2212P(c|y)) Mengukur Jarak Tipe Ordinal \u00b6 Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif denganzif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran \u00b6 Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan d(i,j)=\u2211pf=1\u03b4(f)ijd(f)ij\u2211pf=1\u03b4(f)ijd(i,j)=\u2211f=1p\u03b4ij(f)dij(f)\u2211f=1p\u03b4ij(f) dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, dfij=\u2225xif\u2212xjf\u2225maxhxhf\u2212minhxhfdijf=\u2016xif\u2212xjf\u2016maxhxhf\u2212minhxhf, di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 , dan perlakukan zifzif sebagai numerik. # TUGAS 2 from scipy import stats import pandas as pd df = pd.read_csv('Acute Inflammations.csv',nrows=4) df Temperature of patient Occurrence of nausea Lumbar pain Urine pushing Micturition pains Burning of urethra. itch. swelling of urethra outlet Inflammation of urinary bladder Nephritis of renal pelvis origin 0 35.5 no yes no no no no no 1 35.9 no no yes yes yes yes no 2 35.9 no yes no no no no no 3 36.0 no no yes yes yes yes no binary=[1,2,3,4,5,6,7] ``` from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0], [\"v2-v4\"]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0], [\"v4-v1\"]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) ``` Data Jarak Numeric Binary v1-v2 0 0 0 v1-v3 0 0 0 v2-v3 0 0 0 v2-v4 0 0 0 v3-v4 0 0 0 v4-v1 0 0 0 # Jarak Numeric def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(df.values.tolist()[v1][jnis[x]]**2) normv2=normv2+(df.values.tolist()[v2][jnis[x]]**2) jmlh=jmlh+((df.values.tolist()[v1][jnis[x]])*(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,numerical)]+[0], [\"v1-v3\"]+[0]+[chordDist(0,2,numerical)]+[0], [\"v2-v3\"]+[0]+[chordDist(1,2,numerical)]+[0], [\"v2-v4\"]+[0]+[chordDist(1,3,numerical)]+[0], [\"v3-v4\"]+[0]+[chordDist(2,3,numerical)]+[0], [\"v4-v1\"]+[0]+[chordDist(3,0,numerical)]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 1.4136586205991097 0 v1-v3 0 1.4136586205991097 0 v2-v3 0 1.4136648049944642 0 v2-v4 0 1.4136663296155507 0 v3-v4 0 1.4136663296155507 0 v4-v1 0 1.4136601624057612 0 Jarak Binary def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (df.values.tolist()[v1][jnis[x]])==\"yes\" and (df.values.tolist()[v2][jnis[x]])==\"yes\": q=q+1 elif (df.values.tolist()[v1][jnis[x]])==\"yes\" and (df.values.tolist()[v2][jnis[x]])==\"no\": r=r+1 elif (df.values.tolist()[v1][jnis[x]])==\"no\" and (df.values.tolist()[v2][jnis[x]])==\"yes\": s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,numerical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[chordDist(0,2,numerical)]+[binaryDist(0,2,binary)], [\"v2-v3\"]+[0]+[chordDist(1,2,numerical)]+[binaryDist(1,2,binary)], [\"v2-v4\"]+[0]+[chordDist(1,3,numerical)]+[binaryDist(1,3,binary)], [\"v3-v4\"]+[0]+[chordDist(2,3,numerical)]+[binaryDist(2,3,binary)], [\"v4-v1\"]+[0]+[chordDist(3,0,numerical)]+[binaryDist(3,0,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Jarak def jarak(v1,v2): return ((chordDist(v1,v2,num)+binaryDist(v1,v2,binary))/2) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[\"{:.2f}\".format(jarak(0,1))]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(binaryDist(0,1,binary))], [\"v1-v3\"]+[\"{:.2f}\".format(jarak(0,2))]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(binaryDist(0,2,binary))], [\"v2-v3\"]+[\"{:.2f}\".format(jarak(1,2))]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(binaryDist(1,2,binary))], [\"v2-v4\"]+[\"{:.2f}\".format(jarak(1,3))]+[\"{:.2f}\".format(chordDist(1,3,numerical))]+[\"{:.2f}\".format(binaryDist(1,3,binary))], [\"v3-v4\"]+[\"{:.2f}\".format(jarak(2,3))]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(binaryDist(2,3,binary))], [\"v4-v1\"]+[\"{:.2f}\".format(jarak(3,0))]+[\"{:.2f}\".format(chordDist(3,0,numerical))]+[\"{:.2f}\".format(binaryDist(3,0,binary))], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 1.06 1.41 0.71 v1-v3 0.71 1.41 0.00 v2-v3 1.06 1.41 0.71 v2-v4 0.71 1.41 0.00 v3-v4 1.06 1.41 0.71 v4-v1 1.06 1.41 0.71","title":"WELCOME TO MY WEBSITE"},{"location":"#welcome-to-my-website","text":"CREATE BY : Alful Laila S (NIM : 180411100079)","title":"WELCOME TO MY WEBSITE"},{"location":"#penambangan-data-data-mining","text":"pengertian \u200b Data Mining adalah Serangkaian proses untuk menggali nilai tambah berupa informasi yang selama ini tidak diketahui secara manual dari suatu basisdata dengan melakukan penggalian pola-pola dari data dengan tujuan untuk memanipulasi data menjadi informasi yang lebih berharga yang diperoleh dengan cara mengekstraksi dan mengenali pola yang penting atau menarik dari data yang terdapat dalam basisdata. Data mining biasa juga dikenal nama lain seperti : Knowledge discovery (mining) in databases (KDD), ekstraksi pengetahuan (knowledge extraction) Analisa data/pola dan kecerdasan bisnis (business intelligence) dan merupakan alat yang penting untuk memanipulasi data untuk penyajian informasi sesuai kebutuhan user dengan tujuan untuk membantu dalam analisis koleksi pengamatan perilaku, secara umum definisi data-mining dapat diartikan sebagai berikut Proses penemuan pola yang menarik dari data yang tersimpan dalam jumlah besar. Ekstraksi dari suatu informasi yang berguna atau menarik (non-trivial, implisit, sebefumnya belum diketahui potensial kegunaannya) pola atau pengetahuan dari data yang disimpan dalam jumfah besar. Ekplorasi dari analisa secara otomatis atau semiotomatis terhadap data-data dalam jumlah besar untuk mencari pola dan aturan yang berarti. konsep data mining \u200b *Data mining * sangat perlu dilakukan terutama dalam mengelola Data yang sangat besar untuk memudahkan aktifitas recording suatu transaksi dan untuk proses data warehousing agar dapat memberikan informasi yang akurat bagi penggunanya. Alasan utama mengapa data mining sangat menarik perhatian industri informasi dalam beberapa tahun belakangan ini adalah karena tersedianya data dalam jumlah yang besar dan semakin besarnya kebutuhan untuk mengubah data tersebut menjadi informasi dan pengetahuan yang berguna karena sesuai fokus bidang ilmu ini yaitu melakukan kegiatan mengekstraksi atau menambang pengetahuan dari data yang berukuran/berjumlah besar, informasi inilah yang nantinya sangat berguna untuk pengembangan. berikut langkah-langkahnya : Data cleaning (untuk menghilangkan noise data yang tidak konsisten) Data integration (di mana sumber data yang terpecah dapat disatukan) Data selection (di mana data yang relevan dengan tugas analisis dikembalikan ke dalam database) Data transformation (di mana data berubah atau bersatu menjadi bentuk yang tepat untuk menambang dengan ringkasan performa atau operasi agresi) Knowledge Discovery (proses esensial di mana metode yang intelejen digunakan untuk mengekstrak pola data) Pattern evolution (untuk mengidentifikasi pola yang benar-benar menarik yang mewakili pengetahuan berdasarkan atas beberapa tindakan yang menarik) Knowledge presentation (di mana gambaran teknik visualisasi dan pengetahuan digunakan untuk memberikan pengetahuan yang telah ditambang kepada user).","title":"PENAMBANGAN DATA (DATA MINING)"},{"location":"#tugas-1","text":"\u200b from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df= pd.read_csv(\"penambangan1.csv\") df harga buku berat buku /gram jumlah stok perbulan jumlah terjual bulan ini 0 215319 148 110 25 1 241046 133 139 44 2 146993 73 160 48 3 37237 108 105 50 4 239236 85 128 28 5 148009 123 83 21 6 47522 118 178 50 7 65608 50 90 43 8 43415 106 152 23 9 53957 59 183 26 10 94541 52 58 25 11 213957 149 146 34 12 69540 62 173 33 13 134574 126 137 28 14 145631 96 134 28 15 194378 75 92 50 16 133493 115 132 41 17 222274 107 50 42 18 142645 49 51 23 19 169453 56 165 45 20 85688 134 197 45 21 164231 58 161 42 22 205956 106 78 48 23 126871 121 148 43 24 241294 110 177 35 25 44454 127 156 44 26 132903 57 157 30 27 94979 73 90 45 28 75817 83 95 28 29 61783 64 159 37 ... ... ... ... ... 69 109358 138 73 24 70 163947 153 72 39 71 63159 125 107 23 72 42104 109 145 46 73 80948 95 149 36 74 143200 106 189 31 75 83148 112 162 32 76 147670 144 188 30 77 105098 115 161 22 78 135910 87 70 46 79 87125 71 82 28 80 135804 95 79 42 81 30766 109 166 26 82 87804 76 190 29 83 233446 88 95 26 84 31834 97 103 21 85 54182 112 76 22 86 146553 121 59 42 87 208887 124 57 24 88 207968 101 153 35 89 249144 50 142 30 90 110491 132 141 30 91 219300 126 147 37 92 169832 95 151 33 93 167964 77 200 34 94 70991 92 118 46 95 50295 155 166 43 96 100656 92 71 23 97 212300 122 65 41 98 93128 86 67 31 from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) method harga buku berat buku /gram jumlah stok perbulan jumlah terjual bulan ini describe() count 99.000000 mean 127757.464646 std 61343.035530 min 30435.000000 25% 79288.500000 50% 124700.000000 75% 169176.000000 max 249144.000000 Name: harga buku, dtype: float64 count 99.000000 mean 96.444444 std 28.745620 min 45.000000 25% 73.000000 50% 97.000000 75% 121.000000 max 155.000000 Name: berat buku /gram, dtype: float64 count 99.000000 mean 125.414141 std 42.966529 min 50.000000 25% 88.000000 50% 130.000000 75% 160.500000 max 200.000000 Name: jumlah stok perbulan, dtype: float64 count 99.000000 mean 34.323232 std 8.778253 min 21.000000 25% 28.000000 50% 34.000000 75% 42.000000 max 50.000000 Name: jumlah terjual bulan ini, dtype: float64 count() 99 99 99 99 mean() 127757.464646 96.4444444444 125.414141414 34.3232323232 std() 61343.04 28.75 42.97 8.78 min() 30435 45 50 21 max() 249144 155 200 50 q1() 79288.5 73.0 88.0 28.0 q2() 124700.0 97.0 130.0 34.0 q3() 169176.0 121.0 160.5 42.0 skew() 0.25 -0.02 -0.06 0.18","title":"TUGAS 1"},{"location":"#mengukur-jarak-data","text":"mengukur jarak data numerik Shirkhorshidi, A. S., Aghabozorgi, S., & Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059. Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"MENGUKUR JARAK DATA"},{"location":"#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dmin=( sumni=1|xi\u2212yi|m)1m,m\u22651dmin=( sumi=1n|xi\u2212yi|m)1m,m\u22651 diman mm adalah bilangan riel positif dan xixi dan $ y_i$ adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance\u00b6"},{"location":"#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan dman=n\u2211i=1|xi\u2212yi|dman=\u2211i=1n|xi\u2212yi|","title":"Manhattan distance\u00b6"},{"location":"#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance\u00b6"},{"location":"#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan dave=(1nn\u2211i=1(xi\u2212yi)2)12dave=(1n\u2211i=1n(xi\u2212yi)2)12","title":"Average Distance\u00b6"},{"location":"#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dwe=(n\u2211i=1wi(xi\u2212yi)2)12dwe=(\u2211i=1nwi(xi\u2212yi)2)12dimana wiwi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance\u00b6"},{"location":"#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dchord=(2\u22122\u2211ni=1xiyi\u2225x\u22252\u2225y\u22252)12dchord=(2\u22122\u2211i=1nxiyi\u2016x\u20162\u2016y\u20162)12 dimana \u2225x\u22252\u2016x\u20162 adalah L2-norm\u2225x\u22252=\u221a\u2211ni=1x2iL2-norm\u2016x\u20162=\u2211i=1nxi2","title":"Chord distance\u00b6"},{"location":"#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dmah=\u221a(x\u2212y)S\u22121(x\u2212y)Tdmah=(x\u2212y)S\u22121(x\u2212y)T diman SS adalah matrik covariance data.","title":"Mahalanobis distance\u00b6"},{"location":"#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan Cosine(x,y)=\u2211ni=1xiyi\u2225x\u22252\u2225y\u22252Cosine(x,y)=\u2211i=1nxiyi\u2016x\u20162\u2016y\u20162 dimana \u2225y\u22252\u2016y\u20162 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan \u2225y\u22252=\u221ay21+y22+\u2026+y2n\u2016y\u20162=y12+y22+\u2026+yn2","title":"Cosine measure\u00b6"},{"location":"#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan Pearson(x,y)=\u2211ni=1(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u221a\u2211ni=1(xi\u2212yi)2\u221a\u2211ni=1(xi\u2212yi)2Pearson(x,y)=\u2211i=1n(xi\u2212\u03bcx)(yi\u2212\u03bcy)\u2211i=1n(xi\u2212yi)2\u2211i=1n(xi\u2212yi)2 The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation\u00b6"},{"location":"#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d722\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan jj adalah d(i,j)=r+sq+r+s+td(i,j)=r+sq+r+s+t Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya d(i,j)=r+sq+r+sd(i,j)=r+sq+r+s Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan sim(i,j)=qq+r+s=1\u2212d(i,j)sim\u2061(i,j)=qq+r+s=1\u2212d(i,j) Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary\u00b6"},{"location":"#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical\u00b6"},{"location":"#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan d(x,y)=n\u2211i=1\u03b4(ai(x),ai(y))d(x,y)=\u2211i=1n\u03b4(ai(x),ai(y)) dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y)ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric\u00b6"},{"location":"#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan d(x,y)=n\u2211i=1C\u2211c=1|P(c|ai(x))\u2212P(c|ai(y))|d(x,y)=\u2211i=1n\u2211c=1C|P(c|ai(x))\u2212P(c|ai(y))| dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan d(x,y)=C\u2211c=1|P(c|x)\u2212P(c|y)|d(x,y)=\u2211c=1C|P(c|x)\u2212P(c|y)| diman probabilitas keanggotaan kelas diestimasi dengan P(c|x)P(c|x) dan P(c|y)P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)\u00b6"},{"location":"#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan d(x,y)=C\u2211c=1P(c|x)(1\u2212P(c|y))d(x,y)=\u2211c=1CP(c|x)(1\u2212P(c|y))","title":"Minimum Risk Metric (MRM)\u00b6"},{"location":"#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif denganzif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal\u00b6"},{"location":"#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan d(i,j)=\u2211pf=1\u03b4(f)ijd(f)ij\u2211pf=1\u03b4(f)ijd(i,j)=\u2211f=1p\u03b4ij(f)dij(f)\u2211f=1p\u03b4ij(f) dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, dfij=\u2225xif\u2212xjf\u2225maxhxhf\u2212minhxhfdijf=\u2016xif\u2212xjf\u2016maxhxhf\u2212minhxhf, di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan zif=rif\u22121Mf\u22121zif=rif\u22121Mf\u22121 , dan perlakukan zifzif sebagai numerik. # TUGAS 2 from scipy import stats import pandas as pd df = pd.read_csv('Acute Inflammations.csv',nrows=4) df Temperature of patient Occurrence of nausea Lumbar pain Urine pushing Micturition pains Burning of urethra. itch. swelling of urethra outlet Inflammation of urinary bladder Nephritis of renal pelvis origin 0 35.5 no yes no no no no no 1 35.9 no no yes yes yes yes no 2 35.9 no yes no no no no no 3 36.0 no no yes yes yes yes no binary=[1,2,3,4,5,6,7] ``` from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[0]+[0], [\"v1-v3\"]+[0]+[0]+[0], [\"v2-v3\"]+[0]+[0]+[0], [\"v2-v4\"]+[0]+[0]+[0], [\"v3-v4\"]+[0]+[0]+[0], [\"v4-v1\"]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) ``` Data Jarak Numeric Binary v1-v2 0 0 0 v1-v3 0 0 0 v2-v3 0 0 0 v2-v4 0 0 0 v3-v4 0 0 0 v4-v1 0 0 0 # Jarak Numeric def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(df.values.tolist()[v1][jnis[x]]**2) normv2=normv2+(df.values.tolist()[v2][jnis[x]]**2) jmlh=jmlh+((df.values.tolist()[v1][jnis[x]])*(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,numerical)]+[0], [\"v1-v3\"]+[0]+[chordDist(0,2,numerical)]+[0], [\"v2-v3\"]+[0]+[chordDist(1,2,numerical)]+[0], [\"v2-v4\"]+[0]+[chordDist(1,3,numerical)]+[0], [\"v3-v4\"]+[0]+[chordDist(2,3,numerical)]+[0], [\"v4-v1\"]+[0]+[chordDist(3,0,numerical)]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 0 1.4136586205991097 0 v1-v3 0 1.4136586205991097 0 v2-v3 0 1.4136648049944642 0 v2-v4 0 1.4136663296155507 0 v3-v4 0 1.4136663296155507 0 v4-v1 0 1.4136601624057612 0","title":"Menghitung Jarak Tipe Campuran\u00b6"},{"location":"#jarak-binary","text":"def binaryDist(v1,v2,jnis): q=0 r=0 s=0 t=0 for x in range (len(jnis)): if (df.values.tolist()[v1][jnis[x]])==\"yes\" and (df.values.tolist()[v2][jnis[x]])==\"yes\": q=q+1 elif (df.values.tolist()[v1][jnis[x]])==\"yes\" and (df.values.tolist()[v2][jnis[x]])==\"no\": r=r+1 elif (df.values.tolist()[v1][jnis[x]])==\"no\" and (df.values.tolist()[v2][jnis[x]])==\"yes\": s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[0]+[chordDist(0,1,numerical)]+[binaryDist(0,1,binary)], [\"v1-v3\"]+[0]+[chordDist(0,2,numerical)]+[binaryDist(0,2,binary)], [\"v2-v3\"]+[0]+[chordDist(1,2,numerical)]+[binaryDist(1,2,binary)], [\"v2-v4\"]+[0]+[chordDist(1,3,numerical)]+[binaryDist(1,3,binary)], [\"v3-v4\"]+[0]+[chordDist(2,3,numerical)]+[binaryDist(2,3,binary)], [\"v4-v1\"]+[0]+[chordDist(3,0,numerical)]+[binaryDist(3,0,binary)], ] display(HTML(tabulate.tabulate(table, tablefmt='html')))","title":"Jarak Binary"},{"location":"#jarak","text":"def jarak(v1,v2): return ((chordDist(v1,v2,num)+binaryDist(v1,v2,binary))/2) from IPython.display import HTML, display import tabulate table=[ [\"Data\"]+[\"Jarak\"]+[\"Numeric\"]+[\"Binary\"], [\"v1-v2\"]+[\"{:.2f}\".format(jarak(0,1))]+[\"{:.2f}\".format(chordDist(0,1,numerical))]+[\"{:.2f}\".format(binaryDist(0,1,binary))], [\"v1-v3\"]+[\"{:.2f}\".format(jarak(0,2))]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[\"{:.2f}\".format(binaryDist(0,2,binary))], [\"v2-v3\"]+[\"{:.2f}\".format(jarak(1,2))]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[\"{:.2f}\".format(binaryDist(1,2,binary))], [\"v2-v4\"]+[\"{:.2f}\".format(jarak(1,3))]+[\"{:.2f}\".format(chordDist(1,3,numerical))]+[\"{:.2f}\".format(binaryDist(1,3,binary))], [\"v3-v4\"]+[\"{:.2f}\".format(jarak(2,3))]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[\"{:.2f}\".format(binaryDist(2,3,binary))], [\"v4-v1\"]+[\"{:.2f}\".format(jarak(3,0))]+[\"{:.2f}\".format(chordDist(3,0,numerical))]+[\"{:.2f}\".format(binaryDist(3,0,binary))], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Data Jarak Numeric Binary v1-v2 1.06 1.41 0.71 v1-v3 0.71 1.41 0.00 v2-v3 1.06 1.41 0.71 v2-v4 0.71 1.41 0.00 v3-v4 1.06 1.41 0.71 v4-v1 1.06 1.41 0.71","title":"Jarak"}]}
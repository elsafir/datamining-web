<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="description" content="None">
        
        
        <link rel="shortcut icon" href="img/favicon.ico">
        <title>My Docs</title>
        <link href="css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="css/font-awesome.min.css" rel="stylesheet">
        <link href="css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="js/jquery-1.10.2.min.js" defer></script>
        <script src="js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body class="homepage">

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <a class="navbar-brand" href=".">My Docs</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#welcome-to-my-website">WELCOME TO MY WEBSITE</a></li>
            <li><a href="#penambangan-data-data-mining">PENAMBANGAN DATA (DATA MINING)</a></li>
            <li><a href="#tugas-1">TUGAS 1</a></li>
        <li class="main "><a href="#mengukur-jarak-data">MENGUKUR JARAK DATA</a></li>
        <li class="main "><a href="#mengukur-jarak-data-numerik">mengukur jarak data numerik</a></li>
            <li><a href="#minkowski-distance">Minkowski Distance</a></li>
            <li><a href="#manhattan-distance">Manhattan distance</a></li>
            <li><a href="#euclidean-distance">Euclidean distance</a></li>
            <li><a href="#average-distance">Average Distance</a></li>
            <li><a href="#weighted-euclidean-distance">Weighted euclidean distance</a></li>
            <li><a href="#chord-distance">Chord distance</a></li>
            <li><a href="#mahalanobis-distance">Mahalanobis distance</a></li>
            <li><a href="#cosine-measure">Cosine measure</a></li>
            <li><a href="#pearson-correlation">Pearson correlation</a></li>
            <li><a href="#mengukur-jarak-atribut-binary">Mengukur Jarak Atribut Binary.</a></li>
            <li><a href="#mengukur-jarak-tipe-categorical">Mengukur Jarak Tipe categorical</a></li>
            <li><a href="#mengukur-jarak-tipe-ordinal">Mengukur Jarak Tipe Ordinal</a></li>
            <li><a href="#menghitung-jarak-tipe-campuran">Menghitung Jarak Tipe Campuran</a></li>
            <li><a href="#tugas-2">TUGAS 2</a></li>
            <li><a href="#tugas-3">Tugas 3</a></li>
            <li><a href="#tentang-seleksi-fitur">Tentang Seleksi Fitur</a></li>
        <li class="main "><a href="#naive-bayes">Naive Bayes</a></li>
            <li><a href="#real-test">Real Test</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="welcome-to-my-website">WELCOME TO MY WEBSITE</h1>
<p>CREATE BY : Alful Laila S (NIM : 180411100079)</p>
<h2 id="penambangan-data-data-mining">PENAMBANGAN DATA (DATA MINING)</h2>
<ul>
<li>pengertian</li>
</ul>
<p><strong>Data Mining</strong> adalah Serangkaian proses untuk menggali nilai tambah berupa informasi yang selama ini tidak diketahui secara manual dari suatu basisdata dengan melakukan penggalian pola-pola dari data dengan tujuan untuk memanipulasi data menjadi informasi yang lebih berharga yang diperoleh dengan cara mengekstraksi dan mengenali pola yang penting atau menarik dari data yang terdapat dalam basisdata.</p>
<p><strong>Data mining</strong> biasa juga dikenal nama lain seperti : Knowledge discovery (mining) in databases (KDD), ekstraksi pengetahuan (knowledge extraction) Analisa data/pola dan kecerdasan bisnis (business intelligence) dan merupakan alat yang penting untuk memanipulasi data untuk penyajian informasi sesuai kebutuhan user dengan tujuan untuk membantu dalam analisis koleksi pengamatan perilaku, secara umum definisi data-mining dapat diartikan sebagai berikut</p>
<ul>
<li>Proses penemuan pola yang menarik dari data yang tersimpan dalam jumlah besar.</li>
<li>Ekstraksi dari suatu informasi yang berguna atau menarik (non-trivial, implisit, sebefumnya belum diketahui potensial kegunaannya) pola atau pengetahuan dari data yang disimpan dalam jumfah besar.</li>
<li>Ekplorasi dari analisa secara otomatis atau semiotomatis terhadap data-data dalam jumlah besar untuk mencari pola dan aturan yang berarti.</li>
<li>konsep data mining</li>
</ul>
<p><strong><em>Data mining</em></strong> sangat perlu dilakukan terutama dalam mengelola Data yang sangat besar untuk memudahkan aktifitas recording suatu transaksi dan untuk proses data warehousing agar dapat memberikan informasi yang akurat bagi penggunanya.</p>
<p>Alasan utama mengapa data mining sangat menarik perhatian industri informasi dalam beberapa tahun belakangan ini adalah karena tersedianya data dalam jumlah yang besar dan semakin besarnya kebutuhan untuk mengubah data tersebut menjadi informasi dan pengetahuan yang berguna karena sesuai fokus bidang ilmu ini yaitu melakukan kegiatan mengekstraksi atau menambang pengetahuan dari data yang berukuran/berjumlah besar, informasi inilah yang nantinya sangat berguna untuk pengembangan. berikut langkah-langkahnya :</p>
<ol>
<li>Data cleaning (untuk menghilangkan noise data yang tidak konsisten) Data integration (di mana sumber data yang terpecah dapat disatukan)</li>
<li><em>Data selection</em> (di mana data yang relevan dengan tugas analisis dikembalikan ke dalam database)</li>
<li>Data transformation (di mana data berubah atau bersatu menjadi bentuk yang tepat untuk menambang dengan ringkasan performa atau operasi agresi)</li>
<li>Knowledge Discovery (proses esensial di mana metode yang intelejen digunakan untuk mengekstrak pola data)</li>
<li>Pattern evolution (untuk mengidentifikasi pola yang benar-benar menarik yang mewakili pengetahuan berdasarkan atas beberapa tindakan yang menarik)</li>
<li>Knowledge presentation (di mana gambaran teknik visualisasi dan pengetahuan digunakan untuk memberikan pengetahuan yang telah ditambang kepada user).</li>
</ol>
<h2 id="tugas-1"><strong>TUGAS 1</strong></h2>
<pre><code>from scipy import stats
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
df= pd.read_csv(&quot;penambangan1.csv&quot;)
df
</code></pre>

<table>
<thead>
<tr>
<th align="left">harga buku</th>
<th align="left">berat buku /gram</th>
<th align="left">jumlah stok perbulan</th>
<th align="left">jumlah terjual bulan ini</th>
<th align="left"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">0</td>
<td align="left">215319</td>
<td align="left">148</td>
<td align="left">110</td>
<td align="left">25</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">241046</td>
<td align="left">133</td>
<td align="left">139</td>
<td align="left">44</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">146993</td>
<td align="left">73</td>
<td align="left">160</td>
<td align="left">48</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">37237</td>
<td align="left">108</td>
<td align="left">105</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">4</td>
<td align="left">239236</td>
<td align="left">85</td>
<td align="left">128</td>
<td align="left">28</td>
</tr>
<tr>
<td align="left">5</td>
<td align="left">148009</td>
<td align="left">123</td>
<td align="left">83</td>
<td align="left">21</td>
</tr>
<tr>
<td align="left">6</td>
<td align="left">47522</td>
<td align="left">118</td>
<td align="left">178</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">7</td>
<td align="left">65608</td>
<td align="left">50</td>
<td align="left">90</td>
<td align="left">43</td>
</tr>
<tr>
<td align="left">8</td>
<td align="left">43415</td>
<td align="left">106</td>
<td align="left">152</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left">9</td>
<td align="left">53957</td>
<td align="left">59</td>
<td align="left">183</td>
<td align="left">26</td>
</tr>
<tr>
<td align="left">10</td>
<td align="left">94541</td>
<td align="left">52</td>
<td align="left">58</td>
<td align="left">25</td>
</tr>
<tr>
<td align="left">11</td>
<td align="left">213957</td>
<td align="left">149</td>
<td align="left">146</td>
<td align="left">34</td>
</tr>
<tr>
<td align="left">12</td>
<td align="left">69540</td>
<td align="left">62</td>
<td align="left">173</td>
<td align="left">33</td>
</tr>
<tr>
<td align="left">13</td>
<td align="left">134574</td>
<td align="left">126</td>
<td align="left">137</td>
<td align="left">28</td>
</tr>
<tr>
<td align="left">14</td>
<td align="left">145631</td>
<td align="left">96</td>
<td align="left">134</td>
<td align="left">28</td>
</tr>
<tr>
<td align="left">15</td>
<td align="left">194378</td>
<td align="left">75</td>
<td align="left">92</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">16</td>
<td align="left">133493</td>
<td align="left">115</td>
<td align="left">132</td>
<td align="left">41</td>
</tr>
<tr>
<td align="left">17</td>
<td align="left">222274</td>
<td align="left">107</td>
<td align="left">50</td>
<td align="left">42</td>
</tr>
<tr>
<td align="left">18</td>
<td align="left">142645</td>
<td align="left">49</td>
<td align="left">51</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left">19</td>
<td align="left">169453</td>
<td align="left">56</td>
<td align="left">165</td>
<td align="left">45</td>
</tr>
<tr>
<td align="left">20</td>
<td align="left">85688</td>
<td align="left">134</td>
<td align="left">197</td>
<td align="left">45</td>
</tr>
<tr>
<td align="left">21</td>
<td align="left">164231</td>
<td align="left">58</td>
<td align="left">161</td>
<td align="left">42</td>
</tr>
<tr>
<td align="left">22</td>
<td align="left">205956</td>
<td align="left">106</td>
<td align="left">78</td>
<td align="left">48</td>
</tr>
<tr>
<td align="left">23</td>
<td align="left">126871</td>
<td align="left">121</td>
<td align="left">148</td>
<td align="left">43</td>
</tr>
<tr>
<td align="left">24</td>
<td align="left">241294</td>
<td align="left">110</td>
<td align="left">177</td>
<td align="left">35</td>
</tr>
<tr>
<td align="left">25</td>
<td align="left">44454</td>
<td align="left">127</td>
<td align="left">156</td>
<td align="left">44</td>
</tr>
<tr>
<td align="left">26</td>
<td align="left">132903</td>
<td align="left">57</td>
<td align="left">157</td>
<td align="left">30</td>
</tr>
<tr>
<td align="left">27</td>
<td align="left">94979</td>
<td align="left">73</td>
<td align="left">90</td>
<td align="left">45</td>
</tr>
<tr>
<td align="left">28</td>
<td align="left">75817</td>
<td align="left">83</td>
<td align="left">95</td>
<td align="left">28</td>
</tr>
<tr>
<td align="left">29</td>
<td align="left">61783</td>
<td align="left">64</td>
<td align="left">159</td>
<td align="left">37</td>
</tr>
<tr>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
<td align="left">...</td>
</tr>
<tr>
<td align="left">69</td>
<td align="left">109358</td>
<td align="left">138</td>
<td align="left">73</td>
<td align="left">24</td>
</tr>
<tr>
<td align="left">70</td>
<td align="left">163947</td>
<td align="left">153</td>
<td align="left">72</td>
<td align="left">39</td>
</tr>
<tr>
<td align="left">71</td>
<td align="left">63159</td>
<td align="left">125</td>
<td align="left">107</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left">72</td>
<td align="left">42104</td>
<td align="left">109</td>
<td align="left">145</td>
<td align="left">46</td>
</tr>
<tr>
<td align="left">73</td>
<td align="left">80948</td>
<td align="left">95</td>
<td align="left">149</td>
<td align="left">36</td>
</tr>
<tr>
<td align="left">74</td>
<td align="left">143200</td>
<td align="left">106</td>
<td align="left">189</td>
<td align="left">31</td>
</tr>
<tr>
<td align="left">75</td>
<td align="left">83148</td>
<td align="left">112</td>
<td align="left">162</td>
<td align="left">32</td>
</tr>
<tr>
<td align="left">76</td>
<td align="left">147670</td>
<td align="left">144</td>
<td align="left">188</td>
<td align="left">30</td>
</tr>
<tr>
<td align="left">77</td>
<td align="left">105098</td>
<td align="left">115</td>
<td align="left">161</td>
<td align="left">22</td>
</tr>
<tr>
<td align="left">78</td>
<td align="left">135910</td>
<td align="left">87</td>
<td align="left">70</td>
<td align="left">46</td>
</tr>
<tr>
<td align="left">79</td>
<td align="left">87125</td>
<td align="left">71</td>
<td align="left">82</td>
<td align="left">28</td>
</tr>
<tr>
<td align="left">80</td>
<td align="left">135804</td>
<td align="left">95</td>
<td align="left">79</td>
<td align="left">42</td>
</tr>
<tr>
<td align="left">81</td>
<td align="left">30766</td>
<td align="left">109</td>
<td align="left">166</td>
<td align="left">26</td>
</tr>
<tr>
<td align="left">82</td>
<td align="left">87804</td>
<td align="left">76</td>
<td align="left">190</td>
<td align="left">29</td>
</tr>
<tr>
<td align="left">83</td>
<td align="left">233446</td>
<td align="left">88</td>
<td align="left">95</td>
<td align="left">26</td>
</tr>
<tr>
<td align="left">84</td>
<td align="left">31834</td>
<td align="left">97</td>
<td align="left">103</td>
<td align="left">21</td>
</tr>
<tr>
<td align="left">85</td>
<td align="left">54182</td>
<td align="left">112</td>
<td align="left">76</td>
<td align="left">22</td>
</tr>
<tr>
<td align="left">86</td>
<td align="left">146553</td>
<td align="left">121</td>
<td align="left">59</td>
<td align="left">42</td>
</tr>
<tr>
<td align="left">87</td>
<td align="left">208887</td>
<td align="left">124</td>
<td align="left">57</td>
<td align="left">24</td>
</tr>
<tr>
<td align="left">88</td>
<td align="left">207968</td>
<td align="left">101</td>
<td align="left">153</td>
<td align="left">35</td>
</tr>
<tr>
<td align="left">89</td>
<td align="left">249144</td>
<td align="left">50</td>
<td align="left">142</td>
<td align="left">30</td>
</tr>
<tr>
<td align="left">90</td>
<td align="left">110491</td>
<td align="left">132</td>
<td align="left">141</td>
<td align="left">30</td>
</tr>
<tr>
<td align="left">91</td>
<td align="left">219300</td>
<td align="left">126</td>
<td align="left">147</td>
<td align="left">37</td>
</tr>
<tr>
<td align="left">92</td>
<td align="left">169832</td>
<td align="left">95</td>
<td align="left">151</td>
<td align="left">33</td>
</tr>
<tr>
<td align="left">93</td>
<td align="left">167964</td>
<td align="left">77</td>
<td align="left">200</td>
<td align="left">34</td>
</tr>
<tr>
<td align="left">94</td>
<td align="left">70991</td>
<td align="left">92</td>
<td align="left">118</td>
<td align="left">46</td>
</tr>
<tr>
<td align="left">95</td>
<td align="left">50295</td>
<td align="left">155</td>
<td align="left">166</td>
<td align="left">43</td>
</tr>
<tr>
<td align="left">96</td>
<td align="left">100656</td>
<td align="left">92</td>
<td align="left">71</td>
<td align="left">23</td>
</tr>
<tr>
<td align="left">97</td>
<td align="left">212300</td>
<td align="left">122</td>
<td align="left">65</td>
<td align="left">41</td>
</tr>
<tr>
<td align="left">98</td>
<td align="left">93128</td>
<td align="left">86</td>
<td align="left">67</td>
<td align="left">31</td>
</tr>
</tbody>
</table>
<pre><code>from IPython.display import HTML, display
import tabulate
table=[
        [&quot;method&quot;]+[x for x in df.columns],
        [&quot;describe()&quot;]+['&lt;pre&gt;'+str(df[col].describe())+'&lt;/pre&gt;' for col in df.columns],
        [&quot;count()&quot;]+[df[col].count() for col in df.columns],
        [&quot;mean()&quot;]+[df[col].mean() for col in df.columns],
        [&quot;std()&quot;]+[&quot;{:.2f}&quot;.format(df[col].std()) for col in df.columns],
        [&quot;min()&quot;]+[df[col].min() for col in df.columns],
        [&quot;max()&quot;]+[df[col].max() for col in df.columns],
        [&quot;q1()&quot;]+[df[col].quantile(0.25) for col in df.columns],
        [&quot;q2()&quot;]+[df[col].quantile(0.50) for col in df.columns],
        [&quot;q3()&quot;]+[df[col].quantile(0.75) for col in df.columns],
        [&quot;skew()&quot;]+[&quot;{:.2f}&quot;.format(df[col].skew()) for col in df.columns],
     ]

display(HTML(tabulate.tabulate(table, tablefmt='html')))
</code></pre>

<table>
<thead>
<tr>
<th align="left">method</th>
<th align="left">harga buku</th>
<th align="left">berat buku /gram</th>
<th align="left">jumlah stok perbulan</th>
<th align="left">jumlah terjual bulan ini</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">describe()</td>
<td align="left"><code>count        99.000000 mean     127757.464646 std       61343.035530 min       30435.000000 25%       79288.500000 50%      124700.000000 75%      169176.000000 max      249144.000000 Name: harga buku, dtype: float64</code></td>
<td align="left"><code>count     99.000000 mean      96.444444 std       28.745620 min       45.000000 25%       73.000000 50%       97.000000 75%      121.000000 max      155.000000 Name: berat buku /gram, dtype: float64</code></td>
<td align="left"><code>count     99.000000 mean     125.414141 std       42.966529 min       50.000000 25%       88.000000 50%      130.000000 75%      160.500000 max      200.000000 Name: jumlah stok perbulan, dtype: float64</code></td>
<td align="left"><code>count    99.000000 mean     34.323232 std       8.778253 min      21.000000 25%      28.000000 50%      34.000000 75%      42.000000 max      50.000000 Name: jumlah terjual bulan ini, dtype: float64</code></td>
</tr>
<tr>
<td align="left">count()</td>
<td align="left">99</td>
<td align="left">99</td>
<td align="left">99</td>
<td align="left">99</td>
</tr>
<tr>
<td align="left">mean()</td>
<td align="left">127757.464646</td>
<td align="left">96.4444444444</td>
<td align="left">125.414141414</td>
<td align="left">34.3232323232</td>
</tr>
<tr>
<td align="left">std()</td>
<td align="left">61343.04</td>
<td align="left">28.75</td>
<td align="left">42.97</td>
<td align="left">8.78</td>
</tr>
<tr>
<td align="left">min()</td>
<td align="left">30435</td>
<td align="left">45</td>
<td align="left">50</td>
<td align="left">21</td>
</tr>
<tr>
<td align="left">max()</td>
<td align="left">249144</td>
<td align="left">155</td>
<td align="left">200</td>
<td align="left">50</td>
</tr>
<tr>
<td align="left">q1()</td>
<td align="left">79288.5</td>
<td align="left">73.0</td>
<td align="left">88.0</td>
<td align="left">28.0</td>
</tr>
<tr>
<td align="left">q2()</td>
<td align="left">124700.0</td>
<td align="left">97.0</td>
<td align="left">130.0</td>
<td align="left">34.0</td>
</tr>
<tr>
<td align="left">q3()</td>
<td align="left">169176.0</td>
<td align="left">121.0</td>
<td align="left">160.5</td>
<td align="left">42.0</td>
</tr>
<tr>
<td align="left">skew()</td>
<td align="left">0.25</td>
<td align="left">-0.02</td>
<td align="left">-0.06</td>
<td align="left">0.18</td>
</tr>
</tbody>
</table>
<h1 id="mengukur-jarak-data">MENGUKUR JARAK DATA</h1>
<h1 id="mengukur-jarak-data-numerik">mengukur jarak data numerik</h1>
<p><strong>Shirkhorshidi, A. S., Aghabozorgi, S., &amp; Wah, T. Y. (2015). A comparison study on similarity and dissimilarity measures in clustering continuous data. PloS one, 10(12), e0144059.</strong></p>
<p>Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan</p>
<p>Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya</p>
<h3 id="minkowski-distance"><em>Minkowski Distance</em></h3>
<p>Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan</p>
<p>dmin=( sumni=1|xi−yi|m)1m,m≥1dmin=( sumi=1n|xi−yi|m)1m,m≥1</p>
<p>diman mm adalah bilangan riel positif dan xixi dan $ y_i$ adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.</p>
<h3 id="manhattan-distance"><em>Manhattan distance</em></h3>
<p>Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan</p>
<p>dman=n∑i=1|xi−yi|dman=∑i=1n|xi−yi|</p>
<h3 id="euclidean-distance"><em>Euclidean distance</em></h3>
<p>Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.</p>
<h3 id="average-distance"><em>Average Distance</em></h3>
<p>Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan</p>
<p>dave=(1nn∑i=1(xi−yi)2)12dave=(1n∑i=1n(xi−yi)2)12</p>
<h3 id="weighted-euclidean-distance"><em>Weighted euclidean distance</em></h3>
<p>Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan</p>
<p>dwe=(n∑i=1wi(xi−yi)2)12dwe=(∑i=1nwi(xi−yi)2)12dimana wiwi adalah bobot yang diberikan pada atribut ke i.</p>
<h3 id="chord-distance"><em>Chord distance</em></h3>
<p>Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan</p>
<p>dchord=(2−2∑ni=1xiyi∥x∥2∥y∥2)12dchord=(2−2∑i=1nxiyi‖x‖2‖y‖2)12</p>
<p>dimana ∥x∥2‖x‖2 adalah L2-norm∥x∥2=√∑ni=1x2iL2-norm‖x‖2=∑i=1nxi2</p>
<h3 id="mahalanobis-distance"><em>Mahalanobis distance</em></h3>
<p>Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan</p>
<p>dmah=√(x−y)S−1(x−y)Tdmah=(x−y)S−1(x−y)T</p>
<p>diman SS adalah matrik covariance data.</p>
<h3 id="cosine-measure"><em>Cosine measure</em></h3>
<p>Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan</p>
<p>Cosine(x,y)=∑ni=1xiyi∥x∥2∥y∥2Cosine(x,y)=∑i=1nxiyi‖x‖2‖y‖2</p>
<p>dimana ∥y∥2‖y‖2 adalah Euclidean norm dari vektor y=(y1,y2,…,yn)y=(y1,y2,…,yn) didefinisikan dengan ∥y∥2=√y21+y22+…+y2n‖y‖2=y12+y22+…+yn2</p>
<h3 id="pearson-correlation"><em>Pearson correlation</em></h3>
<p>Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan</p>
<p>Pearson(x,y)=∑ni=1(xi−μx)(yi−μy)√∑ni=1(xi−yi)2√∑ni=1(xi−yi)2Pearson(x,y)=∑i=1n(xi−μx)(yi−μy)∑i=1n(xi−yi)2∑i=1n(xi−yi)2</p>
<p>The Pearson correlation kelemahannya adalah sensitif terhadap outlier</p>
<h2 id="mengukur-jarak-atribut-binary">Mengukur Jarak Atribut Binary.</h2>
<p>Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi.</p>
<p>Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? ”Satu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2×22×2 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t</p>
<p>Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan jj adalah</p>
<p>d(i,j)=r+sq+r+s+td(i,j)=r+sq+r+s+t</p>
<p>Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya</p>
<p>d(i,j)=r+sq+r+sd(i,j)=r+sq+r+s</p>
<p>Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan</p>
<p>sim(i,j)=qq+r+s=1−d(i,j)sim⁡(i,j)=qq+r+s=1−d(i,j)</p>
<p>Persamaan similarity ini disebut dengan <strong>Jaccard coefficient</strong></p>
<h2 id="mengukur-jarak-tipe-categorical">Mengukur Jarak Tipe categorical</h2>
<p><strong>Li, C., &amp; Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.</strong></p>
<p><em>Overlay Metric</em></p>
<p>Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan</p>
<p>d(x,y)=n∑i=1δ(ai(x),ai(y))d(x,y)=∑i=1nδ(ai(x),ai(y))</p>
<p>dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, δ (ai(x),ai(y))δ (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y)ai(x)=ai(y) dan 1 jika sebaliknya.</p>
<p>OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.</p>
<p><em>Value Difference Metric (VDM)</em></p>
<p>VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan</p>
<p>d(x,y)=n∑i=1C∑c=1|P(c|ai(x))−P(c|ai(y))|d(x,y)=∑i=1n∑c=1C|P(c|ai(x))−P(c|ai(y))|</p>
<p>dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y)</p>
<p>VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan</p>
<p>d(x,y)=C∑c=1|P(c|x)−P(c|y)|d(x,y)=∑c=1C|P(c|x)−P(c|y)|</p>
<p>diman probabilitas keanggotaan kelas diestimasi dengan P(c|x)P(c|x) dan P(c|y)P(c|y) didekati dengan Naive Bayes,</p>
<p><em>Minimum Risk Metric (MRM)</em></p>
<p>Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan</p>
<p>d(x,y)=C∑c=1P(c|x)(1−P(c|y))d(x,y)=∑c=1CP(c|x)(1−P(c|y))</p>
<h2 id="mengukur-jarak-tipe-ordinal"><strong>Mengukur Jarak Tipe Ordinal</strong></h2>
<p><strong>Han, J., Pei, J., &amp; Kamber, M. (2011). Data mining: concepts and techniques. Elsevier</strong>.</p>
<p>Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: −30 hingga −10, −10 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf</p>
<p>Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut:</p>
<ul>
<li>Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif∈{1...Mf}rif∈{1...Mf}</li>
<li>Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif denganzif=rif−1Mf−1zif=rif−1Mf−1</li>
<li>Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$</li>
</ul>
<h2 id="menghitung-jarak-tipe-campuran"><strong>Menghitung Jarak Tipe Campuran</strong></h2>
<p><strong>Wilson, D. R., &amp; Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34.</strong></p>
<p>Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan</p>
<p>d(i,j)=∑pf=1δ(f)ijd(f)ij∑pf=1δ(f)ijd(i,j)=∑f=1pδij(f)dij(f)∑f=1pδij(f)</p>
<p>dimana δfij=0δijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj)</p>
<ul>
<li>jika xif=xjf=0xif=xjf=0 dan</li>
<li>atribut ff adalah binary asymmetric,</li>
</ul>
<p>selain itu δfij=1δijf=1</p>
<p>Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya,</p>
<ul>
<li>Jika ff adalah numerik, dfij=∥xif−xjf∥maxhxhf−minhxhfdijf=‖xif−xjf‖maxhxhf−minhxhf, di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f</li>
<li>Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1</li>
<li>Jika ff adalah ordinal maka hitung rangking rifrif dan zif=rif−1Mf−1zif=rif−1Mf−1 , dan perlakukan zifzif sebagai numerik.</li>
</ul>
<h2 id="tugas-2">TUGAS 2</h2>
<pre><code>from scipy import stats
import pandas as pd
df = pd.read_csv('Acute Inflammations.csv',nrows=4)
df
</code></pre>

<table>
<thead>
<tr>
<th align="left"></th>
<th align="left">Temperature of patient</th>
<th align="left">Occurrence of nausea</th>
<th align="left">Lumbar pain</th>
<th align="left">Urine pushing</th>
<th align="left">Micturition pains</th>
<th align="left">Burning of urethra. itch. swelling of urethra outlet</th>
<th align="left">Inflammation of urinary bladder</th>
<th align="left">Nephritis of renal pelvis origin</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">0</td>
<td align="left">35.5</td>
<td align="left">no</td>
<td align="left">yes</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">1</td>
<td align="left">35.9</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">2</td>
<td align="left">35.9</td>
<td align="left">no</td>
<td align="left">yes</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">3</td>
<td align="left">36.0</td>
<td align="left">no</td>
<td align="left">no</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">yes</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<pre><code>  binary=[1,2,3,4,5,6,7]
  from IPython.display import HTML, display
  import tabulate
  table=[
      [&quot;Data&quot;]+[&quot;Jarak&quot;]+[&quot;Numeric&quot;]+[&quot;Binary&quot;],
      [&quot;v1-v2&quot;]+[0]+[0]+[0],
      [&quot;v1-v3&quot;]+[0]+[0]+[0],
      [&quot;v2-v3&quot;]+[0]+[0]+[0],
      [&quot;v2-v4&quot;]+[0]+[0]+[0],
      [&quot;v3-v4&quot;]+[0]+[0]+[0],
      [&quot;v4-v1&quot;]+[0]+[0]+[0],
      ]

  display(HTML(tabulate.tabulate(table, tablefmt='html')))
</code></pre>

<table>
<thead>
<tr>
<th align="left">Data</th>
<th align="left">Jarak</th>
<th align="left">Numeric</th>
<th align="left">Binary</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">v1-v2</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v1-v3</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v2-v3</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v2-v4</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v3-v4</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v4-v1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p><strong>jarak Numeric</strong></p>
<pre><code>def chordDist(v1,v2,jnis):
    jmlh=0
    normv1=0
    normv2=0
    for x in range (len(jnis)):
        normv1=normv1+(df.values.tolist()[v1][jnis[x]]**2)
        normv2=normv2+(df.values.tolist()[v2][jnis[x]]**2)
        jmlh=jmlh+((df.values.tolist()[v1][jnis[x]])*(df.values.tolist()[v2][jnis[x]]))
    return ((2-(2*jmlh/(normv1*normv2)))**0.5)
from IPython.display import HTML, display
import tabulate
table=[
    [&quot;Data&quot;]+[&quot;Jarak&quot;]+[&quot;Numeric&quot;]+[&quot;Binary&quot;],
    [&quot;v1-v2&quot;]+[0]+[chordDist(0,1,numerical)]+[0],
    [&quot;v1-v3&quot;]+[0]+[chordDist(0,2,numerical)]+[0],
    [&quot;v2-v3&quot;]+[0]+[chordDist(1,2,numerical)]+[0],
    [&quot;v2-v4&quot;]+[0]+[chordDist(1,3,numerical)]+[0],
    [&quot;v3-v4&quot;]+[0]+[chordDist(2,3,numerical)]+[0],
    [&quot;v4-v1&quot;]+[0]+[chordDist(3,0,numerical)]+[0],
    ]

display(HTML(tabulate.tabulate(table, tablefmt='html')))
</code></pre>

<table>
<thead>
<tr>
<th align="left">Data</th>
<th align="left">Jarak</th>
<th align="left">Numeric</th>
<th align="left">Binary</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">v1-v2</td>
<td align="left">0</td>
<td align="left">1.4136586205991097</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v1-v3</td>
<td align="left">0</td>
<td align="left">1.4136586205991097</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v2-v3</td>
<td align="left">0</td>
<td align="left">1.4136648049944642</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v2-v4</td>
<td align="left">0</td>
<td align="left">1.4136663296155507</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v3-v4</td>
<td align="left">0</td>
<td align="left">1.4136663296155507</td>
<td align="left">0</td>
</tr>
<tr>
<td align="left">v4-v1</td>
<td align="left">0</td>
<td align="left">1.4136601624057612</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p><strong>JARAK BINARY</strong></p>
<pre><code>def binaryDist(v1,v2,jnis):
    q=0
    r=0
    s=0
    t=0
    for x in range (len(jnis)):
        if (df.values.tolist()[v1][jnis[x]])==&quot;yes&quot; and (df.values.tolist()[v2][jnis[x]])==&quot;yes&quot;:
            q=q+1
        elif (df.values.tolist()[v1][jnis[x]])==&quot;yes&quot; and (df.values.tolist()[v2][jnis[x]])==&quot;no&quot;:
            r=r+1
        elif (df.values.tolist()[v1][jnis[x]])==&quot;no&quot; and (df.values.tolist()[v2][jnis[x]])==&quot;yes&quot;:
            s=s+1
        else:
            t=t+1
    return ((r+s)/(q+r+s+t))
from IPython.display import HTML, display
import tabulate
table=[
    [&quot;Data&quot;]+[&quot;Jarak&quot;]+[&quot;Numeric&quot;]+[&quot;Binary&quot;],
    [&quot;v1-v2&quot;]+[0]+[chordDist(0,1,numerical)]+[binaryDist(0,1,binary)],
    [&quot;v1-v3&quot;]+[0]+[chordDist(0,2,numerical)]+[binaryDist(0,2,binary)],
    [&quot;v2-v3&quot;]+[0]+[chordDist(1,2,numerical)]+[binaryDist(1,2,binary)],
    [&quot;v2-v4&quot;]+[0]+[chordDist(1,3,numerical)]+[binaryDist(1,3,binary)],
    [&quot;v3-v4&quot;]+[0]+[chordDist(2,3,numerical)]+[binaryDist(2,3,binary)],
    [&quot;v4-v1&quot;]+[0]+[chordDist(3,0,numerical)]+[binaryDist(3,0,binary)],
    ]

display(HTML(tabulate.tabulate(table, tablefmt='html')))
</code></pre>

<p>Jarak</p>
<pre><code>def jarak(v1,v2):
    return ((chordDist(v1,v2,num)+binaryDist(v1,v2,binary))/2)
from IPython.display import HTML, display
import tabulate
table=[
    [&quot;Data&quot;]+[&quot;Jarak&quot;]+[&quot;Numeric&quot;]+[&quot;Binary&quot;],
    [&quot;v1-v2&quot;]+[&quot;{:.2f}&quot;.format(jarak(0,1))]+[&quot;{:.2f}&quot;.format(chordDist(0,1,numerical))]+[&quot;{:.2f}&quot;.format(binaryDist(0,1,binary))],
    [&quot;v1-v3&quot;]+[&quot;{:.2f}&quot;.format(jarak(0,2))]+[&quot;{:.2f}&quot;.format(chordDist(0,2,numerical))]+[&quot;{:.2f}&quot;.format(binaryDist(0,2,binary))],
    [&quot;v2-v3&quot;]+[&quot;{:.2f}&quot;.format(jarak(1,2))]+[&quot;{:.2f}&quot;.format(chordDist(1,2,numerical))]+[&quot;{:.2f}&quot;.format(binaryDist(1,2,binary))],
    [&quot;v2-v4&quot;]+[&quot;{:.2f}&quot;.format(jarak(1,3))]+[&quot;{:.2f}&quot;.format(chordDist(1,3,numerical))]+[&quot;{:.2f}&quot;.format(binaryDist(1,3,binary))],
    [&quot;v3-v4&quot;]+[&quot;{:.2f}&quot;.format(jarak(2,3))]+[&quot;{:.2f}&quot;.format(chordDist(2,3,numerical))]+[&quot;{:.2f}&quot;.format(binaryDist(2,3,binary))],
    [&quot;v4-v1&quot;]+[&quot;{:.2f}&quot;.format(jarak(3,0))]+[&quot;{:.2f}&quot;.format(chordDist(3,0,numerical))]+[&quot;{:.2f}&quot;.format(binaryDist(3,0,binary))],
    ]

display(HTML(tabulate.tabulate(table, tablefmt='html')))
</code></pre>

<table>
<thead>
<tr>
<th align="left">Data</th>
<th align="left">Jarak</th>
<th align="left">Numeric</th>
<th align="left">Binary</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">v1-v2</td>
<td align="left">1.06</td>
<td align="left">1.41</td>
<td align="left">0.71</td>
</tr>
<tr>
<td align="left">v1-v3</td>
<td align="left">0.71</td>
<td align="left">1.41</td>
<td align="left">0.00</td>
</tr>
<tr>
<td align="left">v2-v3</td>
<td align="left">1.06</td>
<td align="left">1.41</td>
<td align="left">0.71</td>
</tr>
<tr>
<td align="left">v2-v4</td>
<td align="left">0.71</td>
<td align="left">1.41</td>
<td align="left">0.00</td>
</tr>
<tr>
<td align="left">v3-v4</td>
<td align="left">1.06</td>
<td align="left">1.41</td>
<td align="left">0.71</td>
</tr>
<tr>
<td align="left">v4-v1</td>
<td align="left">1.06</td>
<td align="left">1.41</td>
<td align="left">0.71</td>
</tr>
</tbody>
</table>
<h2 id="tugas-3">Tugas 3</h2>
<h2 id="tentang-seleksi-fitur">Tentang Seleksi Fitur</h2>
<p>Seleksi fitur merupakan salah satu fokus penelitian pada data mining untuk dataset yang memiliki atribut yang relatif banyak. Dengan menghilangkan beberapa atribut yang tidak relevan terhadap kelas label akan dapat meningkatkan kinerja algoritma klasifikasi. Algoritma FVBRM merupakan salah satu algoritma untuk mencari fitur yang tidak relevan terhadap kelas label. Algoritma ini menggunakan teknik wrapper untuk menghilangkan atribut yang tidak relevan. Penelitian ini bertujuan untuk mengimplementasikan seleksi fitur menggunakan algoritma FVBRM terhadap dataset deteksi intrusi NSL KDD yang memiliki jumlah atribut relative banyak. Dataset dengan atribut terpilih akan diuji menggunakan algoritma klasifikasi naive bayes. Evaluasi dilakukan dengan melihat tingkat akurasi klasifikasi yang dihasilkan tanpa seleksi fitur dengan akurasi klasifikasi yang dihasilkan setelah implementasi seleksi fitur. Eksperimen klasifikasi dilakukan dengan dua cara yaitu binary classification (serangan atau bukan serangan) dan lima kelas klasifikasi yaitu dos, r2l, u2r, probe dan normal. Eksperimen dilakukan menggunakan library data mining dilingkungan Weka menggunakan 10 fold validation. Hasil eksperimen menunjukkan bahwa dengan implementasi seleksi fitur menggunakan algoritma FVBRM dapat meningkatkan akurasi klasifikasi menjadi 90,81% untuk dataset dengan 2 kelas label dan 86,55% untuk dataset dengan 5 kelas label.</p>
<p>Algoritma C4.5 merupakan algoritma yang digunakan untuk membentuk pohon keputusan (Decision Tree). Pohon keputusan merupakan metode klasifikasi dan prediksi yang terkenal. Pohon keputusan berguna untuk mengekspolari data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Banyak algoritma yang dapat dipakai dalam pembentukan pohon keputusan, antara lain : ID3, CART, dan C4.5. Algoritma C4.5 merupakan pengembangan dari algoritma ID3, Proses pada pohon keputusan adalah mengubah bentuk data (tabel) menjadi model pohon, mengubah model pohon menjadi rule, dan menyederhanakan rule.</p>
<pre><code>from pandas import *
from IPython.display import HTML, display
from tabulate import tabulate
from math import log
from sklearn.feature_selection import mutual_info_classif
def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False)))
df = read_csv('feature selection.csv', sep=';')
table(df)
</code></pre>

<table>
<thead>
<tr>
<th align="left">outlook</th>
<th align="left">temperature</th>
<th align="left">humidity</th>
<th align="left">windy</th>
<th align="left">play</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">sunny</td>
<td align="left">hot</td>
<td align="left">high</td>
<td align="left">False</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">sunny</td>
<td align="left">hot</td>
<td align="left">high</td>
<td align="left">True</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">overcast</td>
<td align="left">hot</td>
<td align="left">high</td>
<td align="left">False</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">rainy</td>
<td align="left">mild</td>
<td align="left">high</td>
<td align="left">False</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">rainy</td>
<td align="left">cool</td>
<td align="left">normal</td>
<td align="left">False</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">rainy</td>
<td align="left">cool</td>
<td align="left">normal</td>
<td align="left">True</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">overcast</td>
<td align="left">cool</td>
<td align="left">normal</td>
<td align="left">True</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">sunny</td>
<td align="left">mild</td>
<td align="left">high</td>
<td align="left">False</td>
<td align="left">no</td>
</tr>
<tr>
<td align="left">sunny</td>
<td align="left">cool</td>
<td align="left">normal</td>
<td align="left">False</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">rainy</td>
<td align="left">mild</td>
<td align="left">normal</td>
<td align="left">False</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">sunny</td>
<td align="left">mild</td>
<td align="left">normal</td>
<td align="left">True</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">overcast</td>
<td align="left">mild</td>
<td align="left">high</td>
<td align="left">True</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">overcast</td>
<td align="left">hot</td>
<td align="left">normal</td>
<td align="left">False</td>
<td align="left">yes</td>
</tr>
<tr>
<td align="left">rainy</td>
<td align="left">mild</td>
<td align="left">high</td>
<td align="left">True</td>
<td align="left">no</td>
</tr>
</tbody>
</table>
<p>Secara umum algoritma C4.5 untuk membangun pohon keputusan adalah sebagai berikut :</p>
<ul>
<li>Pilih atribut sebagai akar.</li>
<li>Buat cabang untuk tiap-tiap nilai.</li>
<li>Bagi kasus dalam cabang.</li>
<li>Ulangi proses untuk setiap cabang sampai semua kasus pada cabang memiliki kelas yang sama.</li>
</ul>
<p>Konsep Entropy</p>
<ul>
<li>Entropy (S) merupakan jumlah bit yang diperkirakan dibutuhkan untuk dapat mengekstrak suatu kelas (+ atau -) dari sejumlah data acak pada ruang sampel S.</li>
<li>Entropy dapat dikatakan sebagai kebutuhan bit untuk menyatakan suatu kelas.</li>
<li>Entropy digunakan untuk mengukur ketidakaslian S.</li>
</ul>
<p>Konsep Gain</p>
<ul>
<li>Gain (S,A) merupakan perolehan informasi dari atribut A relative terhadap output data S.</li>
<li>Perolehan informasi didapat dari output data atau variable dependent S yang dikelompokkan berdasarkan atribut A, dinotasikan dengan gain (S,A)</li>
</ul>
<p>Langkah 1</p>
<ul>
<li>Menghitung jumlah kasus, jumlah kasus untuk keputusan Yes, jumlah kasus untuk keputusan No, dan Entropy dari semua kasus dan kasus yang dibagi berdasarkan atribut OUTLOOK, TEMPERATURE, HUMIDITY, dan WINDY.</li>
<li>Setelah itu lakukan perhitungan Gain untuk setiap atribut.</li>
<li>Hasil perhitungan ditunjukan di bawah ini.</li>
</ul>
<pre><code>def findEntropy(column):
     rawGroups = df.groupby(column)
     targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups]
     targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability'])
     return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups

entropyTarget, groupTargets, _ = findEntropy('play')
table(groupTargets)
print('entropy target =', entropyTarget)
</code></pre>

<table>
<thead>
<tr>
<th align="left">value</th>
<th align="left">count</th>
<th align="left">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">no</td>
<td align="left">5</td>
<td align="left">0.357143</td>
</tr>
<tr>
<td align="left">yes</td>
<td align="left">9</td>
<td align="left">0.642857</td>
</tr>
</tbody>
</table>
<pre><code>entropy target = 0.9402859586706309
def findGain(column):
    entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column)
    table(groupOutlooks)
    gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) 
                for x in data.groupby('play').size()) for key,data in rawOutlooks)
    print(&quot;gain of&quot;,column,&quot;is&quot;,gain)
    return gain

gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']]
</code></pre>

<table>
<thead>
<tr>
<th align="left">value</th>
<th align="left">count</th>
<th align="left">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">overcast</td>
<td align="left">4</td>
<td align="left">0.285714</td>
</tr>
<tr>
<td align="left">rainy</td>
<td align="left">5</td>
<td align="left">0.357143</td>
</tr>
<tr>
<td align="left">sunny</td>
<td align="left">5</td>
<td align="left">0.357143</td>
</tr>
</tbody>
</table>
<pre><code>gain of outlook is 0.2467498197744391
</code></pre>

<table>
<thead>
<tr>
<th align="left">value</th>
<th align="left">count</th>
<th align="left">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">cool</td>
<td align="left">4</td>
<td align="left">0.285714</td>
</tr>
<tr>
<td align="left">hot</td>
<td align="left">4</td>
<td align="left">0.285714</td>
</tr>
<tr>
<td align="left">mild</td>
<td align="left">6</td>
<td align="left">0.428571</td>
</tr>
</tbody>
</table>
<pre><code>gain of temperature is 0.029222565658954647
</code></pre>

<table>
<thead>
<tr>
<th align="left">value</th>
<th align="left">count</th>
<th align="left">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">high</td>
<td align="left">7</td>
<td align="left">0.5</td>
</tr>
<tr>
<td align="left">normal</td>
<td align="left">7</td>
<td align="left">0.5</td>
</tr>
</tbody>
</table>
<pre><code>gain of humidity is 0.15183550136234136
</code></pre>

<table>
<thead>
<tr>
<th align="left">value</th>
<th align="left">count</th>
<th align="left">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">False</td>
<td align="left">8</td>
<td align="left">0.571429</td>
</tr>
<tr>
<td align="left">True</td>
<td align="left">6</td>
<td align="left">0.428571</td>
</tr>
</tbody>
</table>
<pre><code>gain of windy is 0.04812703040826927
table(DataFrame(gains, columns=[&quot;Feature&quot;, &quot;Gain Score&quot;]).sort_values(&quot;Gain Score&quot;)[::-1])
</code></pre>

<table>
<thead>
<tr>
<th align="left">Feature</th>
<th align="left">Gain Score</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">outlook</td>
<td align="left">0.24675</td>
</tr>
<tr>
<td align="left">humidity</td>
<td align="left">0.151836</td>
</tr>
<tr>
<td align="left">windy</td>
<td align="left">0.048127</td>
</tr>
<tr>
<td align="left">temperature</td>
<td align="left">0.0292226</td>
</tr>
</tbody>
</table>
<h1 id="naive-bayes">Naive Bayes</h1>
<p><strong>Naive Bayes classifier</strong> (NBC) merupakan salah satu metoda <a href="https://id.wikipedia.org/wiki/Pemelajaran_mesin">pemelajaran mesin</a> yang memanfaatkan perhitungan probabilitas dan statistik yang dikemukakan oleh ilmuwan Inggris <a href="https://id.wikipedia.org/w/index.php?title=Thomas_Bayes&amp;action=edit&amp;redlink=1">Thomas Bayes</a>, yaitu memprediksi probabilitas di masa depan berdasarkan pengalaman di masa sebelumnya.</p>
<p>Pengklasifikasi Naif Bayes adalah kumpulan algoritma klasifikasi berdasarkan <strong>Teorema Bayes</strong> . Ini bukan algoritma tunggal tetapi keluarga algoritma di mana mereka semua berbagi prinsip yang sama, yaitu setiap pasangan fitur yang diklasifikasi tidak tergantung satu sama lain.</p>
<p>Untuk memulainya, mari kita pertimbangkan dataset.</p>
<pre><code>from sklearn import datasets
from pandas import *
from numpy import *
from math import *

from IPython.display import HTML, display; from tabulate import tabulate
def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False)))

# IRIS TRAINING TABLE
iris = datasets.load_iris()
data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)]
dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2)
table(dataset)

</code></pre>

<table>
<thead>
<tr>
<th align="right">sepal length (cm)</th>
<th align="right">sepal width (cm)</th>
<th align="right">petal length (cm)</th>
<th align="right">petal width (cm)</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">6.7</td>
<td align="right">3</td>
<td align="right">5.2</td>
<td align="right">2.3</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.6</td>
<td align="right">3</td>
<td align="right">4.4</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.9</td>
<td align="right">4.3</td>
<td align="right">1.3</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.4</td>
<td align="right">1.7</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">2.7</td>
<td align="right">4.2</td>
<td align="right">1.3</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6.2</td>
<td align="right">3.4</td>
<td align="right">5.4</td>
<td align="right">2.3</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.8</td>
<td align="right">2.8</td>
<td align="right">4.8</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">3.4</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.1</td>
<td align="right">4.4</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6.9</td>
<td align="right">3.1</td>
<td align="right">5.1</td>
<td align="right">2.3</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">3.4</td>
<td align="right">4.5</td>
<td align="right">1.6</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">4.4</td>
<td align="right">2.9</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">2.6</td>
<td align="right">5.6</td>
<td align="right">1.4</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.5</td>
<td align="right">1.6</td>
<td align="right">0.6</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">2.5</td>
<td align="right">3</td>
<td align="right">1.1</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">7.1</td>
<td align="right">3</td>
<td align="right">5.9</td>
<td align="right">2.1</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">3.5</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5.9</td>
<td align="right">3</td>
<td align="right">5.1</td>
<td align="right">1.8</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">7</td>
<td align="right">3.2</td>
<td align="right">4.7</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.7</td>
<td align="right">4.1</td>
<td align="right">1</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">2.7</td>
<td align="right">5.1</td>
<td align="right">1.6</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.7</td>
<td align="right">0.4</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.7</td>
<td align="right">5.3</td>
<td align="right">1.9</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.8</td>
<td align="right">5.1</td>
<td align="right">2.4</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.8</td>
<td align="right">5.6</td>
<td align="right">2.2</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.3</td>
<td align="right">5.7</td>
<td align="right">2.1</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.5</td>
<td align="right">5</td>
<td align="right">1.9</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">2.4</td>
<td align="right">3.7</td>
<td align="right">1</td>
<td align="right">versicolor</td>
</tr>
</tbody>
</table>
<pre><code>do = [1,3,5,7]
print(&quot;sampel data: &quot;, do)
</code></pre>

<pre><code>sampel data:  [1, 3, 5, 7]
</code></pre>

<pre><code>dataset_classes = {}
# table per classes
for key,group in dataset.groupby('class'):
    mu_s = [group[c].mean() for c in group.columns[:-1]]
    sigma_s = [group[c].std() for c in group.columns[:-1]]
    dataset_classes[key] = [group, mu_s, sigma_s]
    print(key, &quot;===&gt;&quot;)
    print('Mu_s =&gt;', array(mu_s))
    print('Sigma_s =&gt;', array(sigma_s))
    table(group)

</code></pre>

<pre><code>setosa ===&gt;
Mu_s =&gt; [5.0375 3.35   1.5625 0.275 ]
Sigma_s =&gt; [0.34615232 0.31622777 0.11877349 0.14880476]
</code></pre>

<table>
<thead>
<tr>
<th align="right">sepal length (cm)</th>
<th align="right">sepal width (cm)</th>
<th align="right">petal length (cm)</th>
<th align="right">petal width (cm)</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">5.4</td>
<td align="right">3.4</td>
<td align="right">1.7</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">3.4</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">4.4</td>
<td align="right">2.9</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.5</td>
<td align="right">1.6</td>
<td align="right">0.6</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">3.5</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="right">setosa</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.7</td>
<td align="right">0.4</td>
<td align="right">setosa</td>
</tr>
</tbody>
</table>
<pre><code>versicolor ===&gt;
Mu_s =&gt; [6.13636364 2.85454545 4.29090909 1.31818182]
Sigma_s =&gt; [0.60872445 0.30120968 0.56648839 0.20889319]
</code></pre>

<table>
<thead>
<tr>
<th align="right">sepal length (cm)</th>
<th align="right">sepal width (cm)</th>
<th align="right">petal length (cm)</th>
<th align="right">petal width (cm)</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">6.6</td>
<td align="right">3</td>
<td align="right">4.4</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.9</td>
<td align="right">4.3</td>
<td align="right">1.3</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">2.7</td>
<td align="right">4.2</td>
<td align="right">1.3</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6.8</td>
<td align="right">2.8</td>
<td align="right">4.8</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.1</td>
<td align="right">4.4</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">3.4</td>
<td align="right">4.5</td>
<td align="right">1.6</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">2.5</td>
<td align="right">3</td>
<td align="right">1.1</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">7</td>
<td align="right">3.2</td>
<td align="right">4.7</td>
<td align="right">1.4</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.7</td>
<td align="right">4.1</td>
<td align="right">1</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">2.7</td>
<td align="right">5.1</td>
<td align="right">1.6</td>
<td align="right">versicolor</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">2.4</td>
<td align="right">3.7</td>
<td align="right">1</td>
<td align="right">versicolor</td>
</tr>
</tbody>
</table>
<pre><code>virginica ===&gt;
Mu_s =&gt; [6.40909091 2.92727273 5.36363636 2.06363636]
Sigma_s =&gt; [0.40854509 0.27961012 0.29756588 0.29418609]
</code></pre>

<table>
<thead>
<tr>
<th align="right">sepal length (cm)</th>
<th align="right">sepal width (cm)</th>
<th align="right">petal length (cm)</th>
<th align="right">petal width (cm)</th>
<th align="right">class</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">6.7</td>
<td align="right">3</td>
<td align="right">5.2</td>
<td align="right">2.3</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.2</td>
<td align="right">3.4</td>
<td align="right">5.4</td>
<td align="right">2.3</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.9</td>
<td align="right">3.1</td>
<td align="right">5.1</td>
<td align="right">2.3</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">2.6</td>
<td align="right">5.6</td>
<td align="right">1.4</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">7.1</td>
<td align="right">3</td>
<td align="right">5.9</td>
<td align="right">2.1</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">5.9</td>
<td align="right">3</td>
<td align="right">5.1</td>
<td align="right">1.8</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.7</td>
<td align="right">5.3</td>
<td align="right">1.9</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.8</td>
<td align="right">5.1</td>
<td align="right">2.4</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.8</td>
<td align="right">5.6</td>
<td align="right">2.2</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.3</td>
<td align="right">5.7</td>
<td align="right">2.1</td>
<td align="right">virginica</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.5</td>
<td align="right">5</td>
<td align="right">1.9</td>
<td align="right">virginica</td>
</tr>
</tbody>
</table>
<pre><code>def numericalPriorProbability(v, mu, sigma):
    return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2))))

def categoricalProbability(sample,universe):
    return sample.shape[0]/universe.shape[0]

Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+
          [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()])

table(DataFrame(Ps, columns=[&quot;classes&quot;]+[&quot;P( %d | C )&quot; % d for d in test]+[&quot;P( C )&quot;]))
</code></pre>

<table>
<thead>
<tr>
<th align="left">classes</th>
<th align="right">P( 3 | C )</th>
<th align="right">P( 5 | C )</th>
<th align="right">P( 2 | C )</th>
<th align="right">P( 4 | C )</th>
<th align="right">P( C )</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">setosa</td>
<td align="right">1.96232e-10</td>
<td align="right">2.77721e-09</td>
<td align="right">0.0123515</td>
<td align="right">4.13093e-115</td>
<td align="right">0.2</td>
</tr>
<tr>
<td align="left">versicolor</td>
<td align="right">1.93812e-07</td>
<td align="right">2.31644e-10</td>
<td align="right">2.01329e-05</td>
<td align="right">1.11579e-35</td>
<td align="right">0.4</td>
</tr>
<tr>
<td align="left">virginica</td>
<td align="right">1.07096e-05</td>
<td align="right">4.94165e-07</td>
<td align="right">9.87125e-09</td>
<td align="right">9.46396e-15</td>
<td align="right">0.4</td>
</tr>
</tbody>
</table>
<pre><code>Pss = ([[r[0], prod(r[1:])] for r in Ps])
PDss = DataFrame(Pss, columns=['class','probability']).sort_values('probability')[::-1]
table(PDss)
</code></pre>

<table>
<thead>
<tr>
<th align="left">class</th>
<th align="right">probability</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">virginica</td>
<td align="right">1.97766e-34</td>
</tr>
<tr>
<td align="left">versicolor</td>
<td align="right">4.03412e-57</td>
</tr>
<tr>
<td align="left">setosa</td>
<td align="right">5.56132e-136</td>
</tr>
</tbody>
</table>
<pre><code>print(&quot;Prediksi Bayes untuk&quot;, test, &quot;adalah&quot;, PDss.values[0,0])
Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica
</code></pre>

<h2 id="real-test">Real Test</h2>
<p>Ekstensi Implementasi
Bagian ini memberi Anda ide untuk ekstensi yang dapat Anda terapkan dan selidiki dengan kode Python yang telah Anda terapkan sebagai bagian dari tutorial ini.</p>
<p>Anda telah mengimplementasikan versi Gaussian Naive Bayes Anda sendiri dengan python dari awal.</p>
<p>Anda dapat memperpanjang implementasi lebih lanjut.</p>
<p>Hitung Probabilitas Kelas: Perbarui contoh untuk merangkum probabilitas instance data milik masing-masing kelas sebagai rasio. Ini dapat dihitung sebagai probabilitas instance data milik satu kelas, dibagi dengan jumlah probabilitas instance data milik masing-masing kelas. Misalnya instance memiliki probabilitas 0,02 untuk kelas A dan 0,001 untuk kelas B, kemungkinan instance milik kelas A adalah (0,02 / (0,02 + 0,001)) * 100 yaitu sekitar 95,23%.
Log Probability: Probabilitas kondisional untuk setiap kelas yang diberi nilai atribut kecil. Ketika mereka dikalikan bersama-sama mereka menghasilkan nilai-nilai yang sangat kecil, yang dapat menyebabkan floating point underflow (angka terlalu kecil untuk diwakili dalam Python). Perbaikan umum untuk ini adalah menggabungkan log probabilitas bersama-sama. Teliti dan terapkan perbaikan ini.
Atribut Nominal: Perbarui implementasi untuk mendukung atribut nominal. Ini sangat mirip dan informasi ringkasan yang dapat Anda kumpulkan untuk setiap atribut adalah rasio nilai kategori untuk setiap kelas. Selami referensi untuk informasi lebih lanjut.
Fungsi Kepadatan Berbeda (bernoulli atau multinomial): Kami telah melihat Gaussian Naive Bayes, tetapi Anda juga dapat melihat distribusi lainnya. Menerapkan distribusi yang berbeda seperti multinomial, bernoulli atau kernel naif kernel yang membuat asumsi berbeda tentang distribusi nilai atribut dan / atau hubungannya dengan nilai kelas.</p>
<pre><code># ONE FUNCTION FOR CLASSIFIER

def predict(sampel):
    priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+
          [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()])
    products = ([[r[0], prod(r[1:])] for r in priorLikehoods])
    result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1]
    return result.values[0,0]

dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())'])
table(dataset_test)
</code></pre>

<table>
<thead>
<tr>
<th align="right">sepal length (cm)</th>
<th align="right">sepal width (cm)</th>
<th align="right">petal length (cm)</th>
<th align="right">petal width (cm)</th>
<th align="left">class</th>
<th align="left">predicted class (by predict())</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">5.1</td>
<td align="right">3.5</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.9</td>
<td align="right">3</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.6</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.6</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.7</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.6</td>
<td align="right">3.4</td>
<td align="right">1.4</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.4</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.4</td>
<td align="right">2.9</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.9</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.1</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.7</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.8</td>
<td align="right">3.4</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.8</td>
<td align="right">3</td>
<td align="right">1.4</td>
<td align="right">0.1</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.3</td>
<td align="right">3</td>
<td align="right">1.1</td>
<td align="right">0.1</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">4</td>
<td align="right">1.2</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">4.4</td>
<td align="right">1.5</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.3</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">3.5</td>
<td align="right">1.4</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">3.8</td>
<td align="right">1.7</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">3.8</td>
<td align="right">1.5</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.4</td>
<td align="right">1.7</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">3.7</td>
<td align="right">1.5</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.6</td>
<td align="right">3.6</td>
<td align="right">1</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">3.3</td>
<td align="right">1.7</td>
<td align="right">0.5</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.8</td>
<td align="right">3.4</td>
<td align="right">1.9</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.4</td>
<td align="right">1.6</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">3.5</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">3.4</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.8</td>
<td align="right">3.1</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3.4</td>
<td align="right">1.5</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">4.1</td>
<td align="right">1.5</td>
<td align="right">0.1</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">4.2</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.9</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.2</td>
<td align="right">1.2</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">3.5</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.9</td>
<td align="right">3.6</td>
<td align="right">1.4</td>
<td align="right">0.1</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.4</td>
<td align="right">3</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">3.4</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.5</td>
<td align="right">1.3</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.5</td>
<td align="right">2.3</td>
<td align="right">1.3</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.4</td>
<td align="right">3.2</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.5</td>
<td align="right">1.6</td>
<td align="right">0.6</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">3.8</td>
<td align="right">1.9</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.8</td>
<td align="right">3</td>
<td align="right">1.4</td>
<td align="right">0.3</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">3.8</td>
<td align="right">1.6</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">4.6</td>
<td align="right">3.2</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5.3</td>
<td align="right">3.7</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">3.3</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
<td align="left">setosa</td>
</tr>
<tr>
<td align="right">7</td>
<td align="right">3.2</td>
<td align="right">4.7</td>
<td align="right">1.4</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">3.2</td>
<td align="right">4.5</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.9</td>
<td align="right">3.1</td>
<td align="right">4.9</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">2.3</td>
<td align="right">4</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.5</td>
<td align="right">2.8</td>
<td align="right">4.6</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">2.8</td>
<td align="right">4.5</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">3.3</td>
<td align="right">4.7</td>
<td align="right">1.6</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">4.9</td>
<td align="right">2.4</td>
<td align="right">3.3</td>
<td align="right">1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.6</td>
<td align="right">2.9</td>
<td align="right">4.6</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.2</td>
<td align="right">2.7</td>
<td align="right">3.9</td>
<td align="right">1.4</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">2</td>
<td align="right">3.5</td>
<td align="right">1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.9</td>
<td align="right">3</td>
<td align="right">4.2</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">2.2</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">2.9</td>
<td align="right">4.7</td>
<td align="right">1.4</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">2.9</td>
<td align="right">3.6</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.1</td>
<td align="right">4.4</td>
<td align="right">1.4</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">3</td>
<td align="right">4.5</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.7</td>
<td align="right">4.1</td>
<td align="right">1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.2</td>
<td align="right">2.2</td>
<td align="right">4.5</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">2.5</td>
<td align="right">3.9</td>
<td align="right">1.1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.9</td>
<td align="right">3.2</td>
<td align="right">4.8</td>
<td align="right">1.8</td>
<td align="left">versicolor</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">2.8</td>
<td align="right">4</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.5</td>
<td align="right">4.9</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">2.8</td>
<td align="right">4.7</td>
<td align="right">1.2</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.9</td>
<td align="right">4.3</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.6</td>
<td align="right">3</td>
<td align="right">4.4</td>
<td align="right">1.4</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.8</td>
<td align="right">2.8</td>
<td align="right">4.8</td>
<td align="right">1.4</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3</td>
<td align="right">5</td>
<td align="right">1.7</td>
<td align="left">versicolor</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">2.9</td>
<td align="right">4.5</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">2.6</td>
<td align="right">3.5</td>
<td align="right">1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">2.4</td>
<td align="right">3.8</td>
<td align="right">1.1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">2.4</td>
<td align="right">3.7</td>
<td align="right">1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.7</td>
<td align="right">3.9</td>
<td align="right">1.2</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">2.7</td>
<td align="right">5.1</td>
<td align="right">1.6</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.4</td>
<td align="right">3</td>
<td align="right">4.5</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">3.4</td>
<td align="right">4.5</td>
<td align="right">1.6</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.1</td>
<td align="right">4.7</td>
<td align="right">1.5</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.3</td>
<td align="right">4.4</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">3</td>
<td align="right">4.1</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">2.5</td>
<td align="right">4</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.5</td>
<td align="right">2.6</td>
<td align="right">4.4</td>
<td align="right">1.2</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">3</td>
<td align="right">4.6</td>
<td align="right">1.4</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.6</td>
<td align="right">4</td>
<td align="right">1.2</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5</td>
<td align="right">2.3</td>
<td align="right">3.3</td>
<td align="right">1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">2.7</td>
<td align="right">4.2</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">3</td>
<td align="right">4.2</td>
<td align="right">1.2</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">2.9</td>
<td align="right">4.2</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.2</td>
<td align="right">2.9</td>
<td align="right">4.3</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.1</td>
<td align="right">2.5</td>
<td align="right">3</td>
<td align="right">1.1</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">2.8</td>
<td align="right">4.1</td>
<td align="right">1.3</td>
<td align="left">versicolor</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">3.3</td>
<td align="right">6</td>
<td align="right">2.5</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.7</td>
<td align="right">5.1</td>
<td align="right">1.9</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.1</td>
<td align="right">3</td>
<td align="right">5.9</td>
<td align="right">2.1</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.9</td>
<td align="right">5.6</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.5</td>
<td align="right">3</td>
<td align="right">5.8</td>
<td align="right">2.2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.6</td>
<td align="right">3</td>
<td align="right">6.6</td>
<td align="right">2.1</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">4.9</td>
<td align="right">2.5</td>
<td align="right">4.5</td>
<td align="right">1.7</td>
<td align="left">virginica</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">7.3</td>
<td align="right">2.9</td>
<td align="right">6.3</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">2.5</td>
<td align="right">5.8</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.2</td>
<td align="right">3.6</td>
<td align="right">6.1</td>
<td align="right">2.5</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.5</td>
<td align="right">3.2</td>
<td align="right">5.1</td>
<td align="right">2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.7</td>
<td align="right">5.3</td>
<td align="right">1.9</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.8</td>
<td align="right">3</td>
<td align="right">5.5</td>
<td align="right">2.1</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">5.7</td>
<td align="right">2.5</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.8</td>
<td align="right">5.1</td>
<td align="right">2.4</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">3.2</td>
<td align="right">5.3</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.5</td>
<td align="right">3</td>
<td align="right">5.5</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.7</td>
<td align="right">3.8</td>
<td align="right">6.7</td>
<td align="right">2.2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.7</td>
<td align="right">2.6</td>
<td align="right">6.9</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">2.2</td>
<td align="right">5</td>
<td align="right">1.5</td>
<td align="left">virginica</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.9</td>
<td align="right">3.2</td>
<td align="right">5.7</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">5.6</td>
<td align="right">2.8</td>
<td align="right">4.9</td>
<td align="right">2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.7</td>
<td align="right">2.8</td>
<td align="right">6.7</td>
<td align="right">2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.7</td>
<td align="right">4.9</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.3</td>
<td align="right">5.7</td>
<td align="right">2.1</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.2</td>
<td align="right">3.2</td>
<td align="right">6</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.2</td>
<td align="right">2.8</td>
<td align="right">4.8</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">3</td>
<td align="right">4.9</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.8</td>
<td align="right">5.6</td>
<td align="right">2.1</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.2</td>
<td align="right">3</td>
<td align="right">5.8</td>
<td align="right">1.6</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.4</td>
<td align="right">2.8</td>
<td align="right">6.1</td>
<td align="right">1.9</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">7.9</td>
<td align="right">3.8</td>
<td align="right">6.4</td>
<td align="right">2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">2.8</td>
<td align="right">5.6</td>
<td align="right">2.2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.8</td>
<td align="right">5.1</td>
<td align="right">1.5</td>
<td align="left">virginica</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">6.1</td>
<td align="right">2.6</td>
<td align="right">5.6</td>
<td align="right">1.4</td>
<td align="left">virginica</td>
<td align="left">versicolor</td>
</tr>
<tr>
<td align="right">7.7</td>
<td align="right">3</td>
<td align="right">6.1</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">3.4</td>
<td align="right">5.6</td>
<td align="right">2.4</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.4</td>
<td align="right">3.1</td>
<td align="right">5.5</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6</td>
<td align="right">3</td>
<td align="right">4.8</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.9</td>
<td align="right">3.1</td>
<td align="right">5.4</td>
<td align="right">2.1</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.1</td>
<td align="right">5.6</td>
<td align="right">2.4</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.9</td>
<td align="right">3.1</td>
<td align="right">5.1</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">5.8</td>
<td align="right">2.7</td>
<td align="right">5.1</td>
<td align="right">1.9</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.8</td>
<td align="right">3.2</td>
<td align="right">5.9</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3.3</td>
<td align="right">5.7</td>
<td align="right">2.5</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.7</td>
<td align="right">3</td>
<td align="right">5.2</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.3</td>
<td align="right">2.5</td>
<td align="right">5</td>
<td align="right">1.9</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.5</td>
<td align="right">3</td>
<td align="right">5.2</td>
<td align="right">2</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">6.2</td>
<td align="right">3.4</td>
<td align="right">5.4</td>
<td align="right">2.3</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
<tr>
<td align="right">5.9</td>
<td align="right">3</td>
<td align="right">5.1</td>
<td align="right">1.8</td>
<td align="left">virginica</td>
<td align="left">virginica</td>
</tr>
</tbody>
</table></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = ".",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="js/base.js" defer></script>
        <script src="search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>

<!--
MkDocs version : 1.0.4
Build Date UTC : 2019-10-10 02:38:58
-->
